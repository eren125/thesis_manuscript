%!TeX root = 4-ml.tex
\documentclass[main]{subfiles}

\begin{document}

\chapter{Statistical Learning of Adsorption Properties}
\vspace*{-1\baselineskip}

\section{Machine Learning Models}

Machine learning (ML) models have been widely used to characterize adsorption, transport, catalytic or mechanical properties, just to cite a few. It can in some cases replace very time consuming simulations with simpler calculation of key descriptors that can help the model predict the desired properties. In other cases, it is used to describe the structure--property relationships learned by the ML model. However,machine learning is not a silver bullet, we cannot blindly apply it on any applications; it requires a thorough work on understanding the key variables that will improve the prediction. By using our work on the thermodynamic descriptors and our knowledge on the effect of pressure on the selectivity, we will build a machine learning model to characterize the separation of xenon from krypton at ambient pressure.

\subsection{From algorithm to machine learning}

To understand how machine learning, first, we need to understand how a computer accomplishes a task. The human operator plays a key role in the process --- after designing the solution through theoretical considerations, he needs to write down a list of instructions, called an algorithm, that specifies all needed actions given the circumstances so that the computer achieves the desired outcome. In physical or chemical sciences, these algorithms usually articulate the different components of a theoretical model, which can be an equation without analytical solutions, an analytical expression or a probabilistic problem, just to cite a few. The previous chapters typically presented such algorithms for the simulation of adsorption processes; for instance, the GCMC simulations are based on the statistical physics of the phase equilibrium between a gas phase and adsorption phase inside a nanoporous material, and a Monte Carlo model is used to reproduce the statistics associated to the grand canonical ensemble. The energy sampling algorithms, along with the Widom insertion, are also good examples of how the computer can help the theoretician model the systems under specific chemical and physical conditions. 

A machine learning model is also based on an algorithm, but the goal is very different from the above-mentioned examples --- it doesn't aim at giving all the details of the computation according to proven theoretical principles. As implied by the name, the ambition would be to learn underlying relations within the input data so that it performs the task itself. The machine learning (ML) algorithm is then the list of instructions that specifies how the machine is going to learn from the data. For example, clustering algorithms can distinguish different classes of elements within a disordered dataset so that new concepts emerge; this type of machine learning algorithm is called unsupervised learning because we do not predefine or pre-label the data and the machine helps us understand the underlying structure in the data; we will not go deeper in the details of this type of algorithm since it goes beyond the frame of this thesis. The class of algorithm we want to study is rather the supervised learning model, which learns from labeled data the relation between the label and the characteristics (called features or descriptors) from a given set of data points, and can predict the label from unlabeled data using the characteristics. For example, if we want to predict the weather of tomorrow, the model could use the past weather of similar dates to infer if it will rain tomorrow; the history of the weather  forms the features of the ML model and the future weather is the target variable or the label of the data.

To articulate the differences between a standard algorithm and an ML algorithm, let me introduce a fascinating board game called Go. This game is traditionally played with 2 players on a 19 by 19 board, where each player places black/white pieces to control the maximum of boxes. Based on these simple rules, different algorithms have been developed to make the computer play the game. The first Go program was written in the late 60s to mimic the pattern recognition of Go players when estimating the ``score'' through an influence function,\autocite{zobrist1970feature} and from the 80s to the beginning of the 21\ex{st} century the first Go programs capable of playing were releases. These programs were based on simple alpha-beta search algorithms that seeks at testing every possible move (while pruning the less promising ones); while they were working very well in other games like chess (IBM's Deep(er) Blue beat the world champion of chess in 1995), in Go these types of programs were only at the level of a novice player. The difference of performance lies in the combinatorics behind both games, the game of chess has a number of legal positions lower than $10^{47}$,\autocite{website_labelle} while for the game of Go there are approximately $10^{171}$ legal positions.\autocite{Tromp_2007,github_tromp_go} The state space to explore is incomparably greater and a boost in the computing power that improved the performance for computer chess is not going to make a difference for Go. A drastic reduction of the space to be explored is needed for a computer program to work. The biggest improvement came, when in 2007, Coulom introduced a Monte Carlo tree search.\autocite{Coulom_2007} This algorithm uses heuristics to distinguish between bad a good move according to human perception of the game, a probability of selection is assigned to the moves according to their potential (policy), potential moves are randomly picked according to this probability; the average outcomes associated with a parent move gives the value of the move. The computer Go is now more efficient in the evaluation of the moves using a Monte Carlo sampling, and it can now play with average amateur players, but it is nowhere near surpassing them. Up until now, the algorithms are based on human knowledge that the programmer implements directly in the computer using machine instructions. Statistics and randomness are used to orient the machine towards the best moves and reduce their predictability, but the statistics that identifies the moves are based on human heuristics that are usually not generalizable. The big revolution brought about by machine learning in the field aims at better evaluating these statistics using the data from already played games. By using a dataset of 30 million moves, the Alpha Go is based on the same Monte Carlo tree search framework but it replaces the formulas behind the probability of searching a move by a machine learning model called the ``policy network'' and the one behind evaluating the confidence in winning of the position by a value ``network''.\autocite{Silver_2016} Alpha Go was the first computer program to beat a world champion in 2016. One year later, to further emancipate from human knowledge, an improved version, Alpha Go zero generates its own data by playing games against itself to train a similar machine learning structure than presented before. This new version beats 100 times out of 100 the former version,\autocite{Silver_2017} which marks a new era of domination of computer go over the best player in the world, and the defeat of another top player just confirms the advent of this new era. 

In this example, we can see how the machine learned the value of each moves by compiling the knowledge of huge datasets in a deep neural network. The main difference between conventional approaches to algorithmic and machine learning is very well illustrated in the previous example; the goal is not to tell the computer how to play using player knowledge implemented in formulas and explicit instructions, but it is to give an explicit framework with flexible parameters that the model needs to learn using a database. In other words, the parameters of a model are fitted to match the values of a database, while being capable of generalizing in situations outside of the database (this notion of generalizability will be further discussed in the following sections). In this section, the goal is not to give a complete overview of all existing models but rather to introduce the main concepts of ML through the example of the model we will use for our problem of selectivity performance prediction.

\subsection{Introduction to supervised learning}

In this thesis, we will focus on the most common way of statistically learning from data, which is the supervised learning. As previously introduced, supervised learning corresponds to the extraction of a relationship between the labels of a set of data points and some of their known characteristics or features. This relationship can be called the model or the predictor and should ideally generalize to similar but unseen data. In this section, we will formalize the goal of the learning algorithm when given a set of labeled data in order to introduce more complex notions in machine learning like the bias--variance trade-off and also more specific models that will be used in this chapter like the tree-based models. Different books have been used for the conception of this section, mainly the Elements of Statistial Learning\autocite{Hastie_2009} and an Introduction to machine learning (in French) from Azencott\autocite{azencott2022introduction}.

\subsubsection{Mathematical considerations}

In supervised learning, the algorithm learns from a set of data noted $\mathcal{D}_{n}=\left\{(\mathbf{x}_{1},y_{1}),...,(\mathbf{x}_{n},y_{n})\right\}$ with $n$ observed data points, where $\mathbf{x}_{i}$ represents an input observable which is a vector of $\mathbb{R}^{p}$ ($p=1$ for scalars) and $y_{i}$ is the label of the data point $i$ that belongs to a set $\mathcal{Y}$ (numerical, categorical or vectorial). 
We can model the statistical observables by a random variable $X$ and the label by another random variable $Y$. The dataset only gives a parcelar view of the joint probability (see equation~\ref{eq:joint_proba}) and the goal will be to extrapolate the relation to unseen data. $(X,Y)$ represents all possible combinations of data points seen an unseen.
\begin{equation}\label{eq:joint_proba}
  \forall\ \mathbf{x}\in\mathbb{R}^{p},\ y\in\mathcal{Y},\ \mathbb{P}\left(X=\mathbf{x}, Y=y\right) = \mathbb{P}(X=\mathbf{x})\mathbb{P}(Y=y|X=\mathbf{x})
\end{equation}
The challenge of supervised learning lies in the fact that the data at our disposal does not give a complete picture of the probability law. And the goal is to give the most probable label $y$ for a data point characterized by $\mathbf{x}$, which is determining the conditional expectation $\E{Y|X=\mathbf{x}}$ of $Y$ given the observable $\mathbf{x}$, which depends on the conditional probabilities $\mathbb{P}(Y=y_i|X=\mathbf{x}_j)$ seen across all data points $i,j\in\{1,\ldots,n\}$.

\begin{figure}[ht]
  \centering
    \includegraphics[width=0.9\textwidth]{figures/4-ml/machine learning.pdf}
    \caption{Illustration of the basic principle of supervised learning.}\label{fgr:supervised_leaning}
\end{figure}

To do so, the learning algorithm uses a ``predictor'' $f$ that can be defined as the function that associates values (features) from $\mathcal{X}=\mathbb{R}^{p}$ to values of $\mathcal{Y}$. By changing the learning model (subsection~\ref{sct:model}) or by changing the feature space $\mathcal{X}$ we can define different domains $\mathcal{F}\subseteq{\mathcal{Y}}^{\mathcal{X}}$ where we search for the prediction function $f$. The domain $\mathcal{F}$ can either be too restrictive and the optimum function found is too far from the theoretical one, or be too large and the optimization problem is nearly impossible to solve or the solution is way too close to the data, which raises question of fitting that will be discussed later.

This predictor can be interpreted as the function that gives the most probable outcome $y$ for a given input $\mathbf{x}$. To evaluate the quality of the predictor, we can introduce a loss function $\mathcal{L}:\mathcal{Y}\times\mathcal{Y} \rightarrow \mathbb{R}^{p}$ that compares the predicted value $f(\mathbf{x})$ to the true value $y$ on the dataset at our disposal $\mathcal{D}_{n}$. This loss function needs to increase when $f(\mathbf{x})$ moves away from $y$. To extend the definition of the loss to the entire possible space, we can introduce the theoretical risk $\mathcal{R}$ of a predictor $h$ using the random variables $X$ and $Y$ so that $\mathcal{R}(h) = \E{\mathcal{L}\left(h(X),Y\right)}$. However, since we do not know the exact mapping of the random variables, we will rather evaluate empirically the risk $\mathcal{R}_n$ on the known dataset $\mathcal{D}_{n}$:
\begin{equation}\label{eq:risk}
  \mathcal{R}_n(h) = \frac{1}{n}\sum_{i=1}^n \mathcal{L}\left(h(\mathbf{x}_i),y_i\right)
\end{equation}

The goal is therefore to find a function that minimizes the risk function across the known data, and this optimal predictor $f^*$ can be defined as:
\begin{equation}\label{eq:min_f}
  f_n^* = \underset{f\in\mathcal{F}}{\text{arg min}} \mathcal{R}_n(f)
\end{equation}


Many loss functions can be used and depending on the definition, we enphasize more or less on the larges errors. For instance a quadratic cost function will penalize a lot the outlier so that a few medium errors are better than one great error, which is not the case for an absolute cost. Since we only use regression models in this thesis, we will not go into the details of classification loss functions and will rather focus more on the regression loss functions. The quadratic loss or squared error loss $L\e{SE}(f(\mathbf{x}),y)=0.5\left(y-f(\mathbf{x})\right)^2$ of a predictor $f$ on a data point $(\mathbf{x},y)$ is simply defined as the squared difference between the prediction and the true label. The multiplicative $0.5$ coefficient is here to simplify in the derivatives. This loss is similar to the mean squared error (MSE) used to compare two quantities across a dataset, the risk function actually corresponds to half of the MSE on the predictions $\mathcal{D}_n$:
\begin{equation}
  \mathcal{R}\e{SE}(f) = 0.5\frac{1}{n}\sum_{i=1}^n \left(y_i-f(\mathbf{x}_i)\right)^2
\end{equation}

A second very common loss function is the absolute loss, which is associated with the mean absolute error (MAE) used in error evaluation. The loss can be expressed as $L\e{AE}(f(\mathbf{x}),y)=\left|y-f(\mathbf{x})\right|$, and the risk function associated is simply the MAE across the dataset predictions:
\begin{equation}
  \mathcal{R}\e{AE}(f) = \frac{1}{n}\sum_{i=1}^n \left|y_i-f(\mathbf{x}_i)\right|
\end{equation}
It is also possible to make this loss function flatter near the minimal error by introducing a parameter $\epsilon$. The $\epsilon$-insensitive loss corresponds to a modified absolute loss $L_{\epsilon}(f(\mathbf{x}),y)=\max\left(0,\left|y-f(\mathbf{x})\right|\right)$.

Finally, it is possible to combine the less outlier-sensitive absolute loss with the smoothness of the quadratic loss near the minimal error domain by using a Huber loss. For a given $\delta$, the Huber loss is defined as:
\begin{equation}
  L_{\delta}(f(\mathbf{x}),y) = \left\{
    \begin{array}{ll}
        \tfrac{1}{2}\left(y-f(\mathbf{x})\right)^2 & \mbox{for } \left|y-f(\mathbf{x})\right| \leq \delta \\
        \delta\left(\left|y-f(\mathbf{x})\right| - \tfrac{1}{2}\delta\right) & \mbox{otherwise.}
    \end{array}
  \right.
\end{equation}

We can see the different loss function on the Figure~\ref{fgr:loss_comp}.\todo{comment figure and link all errors}

\begin{figure}[ht]
  \centering
    \includegraphics[width=0.6\textwidth]{figures/4-ml/loss_comparison.pdf}
    \caption{Comparison of different loss functions (quadratic loss, absolute loss, $\epsilon$-insensitive and the Huber loss). }\label{fgr:loss_comp}
\end{figure}

Through these theoretical considerations, we demystified the process of a machine learning from data by simply formulating this leaning process as an optimization of a cost function, which is a common tool in any scientific field. However, this optimization problem is challenging in the sense that the variable is a function that lies in a high dimension space and approximations are needed to reduce the space. This is why most of the engineering breakthroughs happens in the conception of the architecture of the ML model that defines the form of the prediction function $f$. Another difficulty of machine learning is to deal with an ill-posed problem, which means that one of the three conditions of Hadamard is not verified. These conditions being the existence of a solution, its unicity and the continuity of the solution with regards to the initial conditions. This issue is usually tackled using regularization techniques such as the one introduced by Tikhonov in the second half of the 20\e{th} century. Furthermore, the minization of the empiric risk is not always consistant with the minization of the more global risk (considering all possible observations), since minimizing $\mathcal{R}_n$ does not always give the same solution as the minimization of $\mathcal{R}$. Therefore, the complexity of the risk optimization problem depends on the loss function chosen but also the domain $\mathcal{F}$ defined by the model and different techniques can be used to construct a solution without anything that guarantees the optimality of it. One of the biggest challenge of ML is to overcome the problem of generalizability, which will be the topic of the next discussion.


\subsubsection{Generalization and overfitting}

As previously discussed, the optimization problem is ill-defined and we have no guarantee for the model to work on other data points as $n$ goes toward infinite. The generalizability of model consists in ensuring the predictability of unseen data so that the solution does not simply correspond to the minimal risk for the data $\mathcal{D}_{n}$ but also for other $m$ data points $\left\{(\mathbf{x}_{n+1},y_{n+1}),\ldots,(\mathbf{x}_{n+m},y_{n+m})\right\}$ all different from the previous set. One of the main phenomenon that explains this discrepancy between the solution $f_n^*$ and the ideal solution $f^*$ (considering an infinite amount of data) is the noise in the dataset. The data is not perfectly measured, and the uncertainty attached to each $\mathbf{x}_i$ and $y_i$ values can create a residual noise that needs to be ignored in the learning process. Moreover, sometimes, the $p$ explanatory variables considered are not enough to model the target phenomenon. To train a generalizable model, we would need to learn enough to capture the inner relation between $X$ and $Y$, but it should not fit the data too closely and capture the noise along the way, otherwise we say that the model overfits the data. If the model is very inaccurate even on the training data, we say that the model underfits, and generally it means that the model is too simplistic (not enough features or too low-level architecture). 

This problem of overfitting can be summarized in the fundamental notion of bias--variance tradeoff in machine learning and more generally in statistics. The error can be broken down in two types: the bias error measures the error made on the available data $\mathcal{D}_n$, while the variance error measures the sensibility to small variations in the input values. A high bias error corresponds to an underfitting, we did not learn enough from the data; and a high variance error means an overfitting, we learned too much even superfluous relations. To formalize these errors, we can go back to the empiric risk function $\mathcal{R}_n(f)$ that models the error of our predictor $f\in\mathcal{F}$; to know if we reach the ideal optimum we would need to compare it to the minimal risk that a predictor with infinite knowledge would get $\mathcal{R}^* = \underset{\scriptscriptstyle h\in\mathcal{Y}^{\mathcal{X}}} {\text{min}}\mathcal{R}(h)$. This excess error $\mathcal{R}_n(f)-\mathcal{R}^*$ can then be broken down in two errors that could be interpreted as the bias and the variance errors:
\begin{equation}
  \mathcal{R}_n(f)-\mathcal{R}^* = \left[\mathcal{R}_n(f) - \underset{\scriptstyle h\in\mathcal{F}} {\text{min}}\mathcal{R}_n(h)\right] + \left[\underset{\scriptstyle h\in\mathcal{F}} {\text{min}}\mathcal{R}_n(h) -\mathcal{R}^*\right]
\end{equation}
The first term of the above-written sum corresponds to a bias error, because it measures how far the current predictor $f$ is off of a minimum (there can be several in an ill-posed problem) risk predictor $f_n^*$ determined using the $n$ data points. The second term, on the other hand, is the residual error associated with the choice of the predictor domain $\mathcal{F}$ and the fact that only a finite amount of data is accessible to the prediciton model. With an infinite amount of data, the model $f^*$ associated with the risk $\mathcal{R}^*$ would not be influenced by the noise since several data points with similar features but with small noises would give a similar prediction. The difference of loss between this ideal function $f^*$ and the current function we are testing $f$ would correspond to an overfitting of the noise that could not be distinguished in the finite case, if we consider the domain $\mathcal{F}$ defined by the model suitable. On the contrary, if there is a problem of model, this error also measures the approximation error due to the choice of a given set of features with a given model architecture.

\begin{figure}[ht]
  \centering
    \includegraphics[width=0.45\textwidth]{figures/4-ml/Bias_and_variance_contributing_to_total_error.svg.png}
    \includegraphics[width=0.45\textwidth]{figures/4-ml/Overfitted_Data.png}
    \caption{On the left, theoretical relation between the bias, variance and total errors and the model complexity taken from \href{https://en.wikipedia.org/wiki/File:Bias_and_variance_contributing_to_total_error.svg}{Wikimedia Commons} under above \href{https://creativecommons.org/licenses/by-sa/4.0}{CC BY-SA 4.0} license. This is an illustration of what happens for instance on the left plot (taken from \href{https://en.wikipedia.org/wiki/File:Overfitted_Data.png}{Wikimedia Commons} under a \href{https://creativecommons.org/licenses/by-sa/4.0}{CC BY-SA 4.0} license), when considering different degrees of polynomials. The lower degree linear function is more generalizable that the biased high degree polynomial that fits perfectly the data.}\label{fgr:bias_variance}
\end{figure}

In general, if the model is very complex in comparison to the amount of data we have, we would fit too closely to the data and have a very high change to overfit. The opposite is also true, a simplistic model would yield to a poor bias error and the model would be underfit. This principle is represented on the Figure~\ref{fgr:bias_variance} and should guide us in the design of a new ML model.
The complex art of fitting a model to a dataset consists in finding the right balance between the bias and the variance. Fortunately, some optimization tools can help us reduce the variance error by changing the loss function itself, and we are going to look into them in the next part of our discussion.

\subsubsection{Regularization to fight against overfitting}

Regularization consists generally in adding implicit or explicit constraints on the optimization problem to find not only the most accurate solution (minimal loss) but also the simplest. This criterion of simplicity is crucial in the generalization of the problem. We typically don't need a high degree polynomial when a linear function is a more suitable solution as shown on Figure~\ref{fgr:bias_variance}. 

The explicit regularization technique consists in penalizing the complexity of a model by adding to the global loss function an error term that scales with the complexity of the model. The error associated to a predictor $f$ can be expressed with an additional regularization term ${\Omega}_n(f)$:
\begin{equation}
  \mathcal{R}_n(f) = \frac{1}{n}\sum_{i=1}^n \mathcal{L}\left(f(\mathbf{x}_i),y_i\right) + \Omega_n(f)
\end{equation}
And, depending on the expression of the regularization term ${\Omega}_n(f)$, the regularization will have more or less influence on the optimization problem. 

Since the regularization is a model-specific function (depends on $f$), we need to define a model to study more specific expressions of regularization. 
Let's consider a multilinear model so that $f(\mathbf{x}) = \mathbf{w}\mathbf{x}\ex{T}$, where $\mathbf{w}=(w^{(1)},\ldots,w^{(p)})$ is a vectorial representation of the weights of the $p$ features contained in $\mathbf{x}$ in the linear regression. In a standard multilinear regression, with a quadratic loss, the risk function to minimize can be expressed as:
\begin{equation}\label{eq:linear}
  \mathcal{R}_n(f) = \frac{1}{n}\sum_{i=1}^n \left(\mathbf{w}\mathbf{x}_i\ex{T}-y_i\right)^2 
\end{equation}
and $y_i$ is now a scalar in a regression problem ($\mathcal{Y}=\mathbb{R}$). One of the earliest regularization tool introduced by Tikhonov to deal with ill-posed optimization problem is the L2 regularization. Used in linear regression, this new type of model is called the ridge regression and consists simply in adding a L2-norm penality on the model weights in the risk function as expressed in the following equation:
\begin{equation}\label{eq:L2_reg}
  \mathcal{R}_n(f) = \frac{1}{n}\sum_{i=1}^n \mathcal{L}\left(f(\mathbf{x}_i),y_i\right) + \lambda_2 {\lVert \mathbf{w} \rVert}_2^2 = \frac{1}{n}\sum_{i=1}^n \left(\mathbf{w}\mathbf{x}_i\ex{T}-y_i\right)^2 + \lambda_2 \sum_{k=1}^p \left|w^{(k)}\right|^2
\end{equation}
where $\lambda_2$ is the prameter of the L2-regularization, it controls the importance of the regularization term in the optimization process. This parameter controls the complexity of the model and needs to be tweaked to find the optimum between accuracy and generalizability as shown on the Figure~\ref{fgr:bias_variance}. If we now consider a polynomial function, the vector $\mathbf{x}_i$ represents the vector of different exponentiations of a scalar $x_i$ so that $\mathbf{x}_i=\left(x_i^0,\ldots,x_i^{n-1}\right)$, and the weights $\mathbf{w}$ are just the polynomial coefficients of the polynom $f$. This is a clear illustration of how the complexity of the model is penalized since regularization terms directly measures how many terms are used and how influencial they are in the fitting process.
Note that this regularization can be adapted to other types of model, given that we manage to define a L2-norm of the prediction function $f$.

A second very common regularization term is based on the L1-norm of the prediction function. A L1-regularized least square linear regression is called a LASSO (Least Absolute Shrinkage and Selection Operator) regression, it allows for a sparser selection of the model weights by allowing zero weights in the model, which is not the case for a L2-regularization. The risk function associated with this regression model can be expressed as:
\begin{equation}\label{eq:L1_reg}
  \mathcal{R}_n(f) = \frac{1}{n}\sum_{i=1}^n \mathcal{L}\left(f(\mathbf{x}_i),y_i\right) + \lambda_1{\lVert \mathbf{w} \rVert}_1 = \frac{1}{n}\sum_{i=1}^n \left(\mathbf{w}\mathbf{x}_i\ex{T}-y_i\right)^2 + \lambda_1 \sum_{k=1}^p |w^{(k)}|
\end{equation}
where $\lambda_1$ is the L1-regularization parameter that controls its importance. The L1-norm can be defined differently depending on the model we consider, but the core idea is that it is a function of the absolute values of the weights of the model. 

Finally, if we combine both L1 and L2-regularization, the linear regression becomes an elastic net regression and the risk function becomes:
\begin{equation}\label{eq:elasticnet_reg}
  \mathcal{R}_n(f) = \frac{1}{n}\sum_{i=1}^n \mathcal{L}\left(f(\mathbf{x}_i),y_i\right) + \lambda_{1,2} \left({\alpha \lVert \mathbf{w} \rVert }_1 + (1-\alpha) {\lVert \mathbf{w} \rVert}_2^2\right)
\end{equation}
where $\alpha\in[0,\,1]$ defines the relative weight of L1 and L2 regularization term and $\lambda_{1,2}$ governs the importance of the combined regularization term. This regularization technique simply combines both L1 and L2 regularization, and the different regularization parameters can be tweaked to find the best bias--variance tradeoff for the final model. These parameters are also called hyperparameters in machine learning, because it changes the parameters at the higher model level.

Finally, implicit regularization corresponds to other forms of control of the complexity of the model. For instance, it could be the early stopping in a learning process so that we do not converge completely to the minimal error with the data. It could be discarding outliers that prevent the model from learning properly on the relevant data. It could also be in the architecture of the model, for instance the random forest is an ensemble approach that aims at reducing the overfit, and it will be presented in the next section. The learning rate in the gradient boosting is also a regularization parameter that smoothes the learning process and will be tackled in the dedicated section. The implicit regularization is related to the construction of the model itself and will therefore be explained in more details in the section on machine learning models.

\subsubsection{Learning strategies}

We previously identified the theory behind the bias--variance tradeoff, which boils down to the generalization of model that has a partial glimpse of all the available data. Yet, in practice we need to evaluate the generalization error $\mathcal{R}_n(f)-\mathcal{R}^*$. To achieve that, the common strategy is to randomly split the available data into two sets a training set $\mathcal{D}\e{train}=\left\{(\mathbf{x}_{i_1},y_{i_1}),...,(\mathbf{x}_{i_t},y_{i_t})\right\}$ and a test set $\mathcal{D}\e{test}=\left\{(\mathbf{x}_{j_1},y_{j_1}),...,(\mathbf{x}_{j_{n-t}},y_{j_{n-t}})\right\}$ so that $\mathcal{D}_n = \mathcal{D}\e{train} \cap \mathcal{D}\e{test}$. The training set is used to perform the optimization problem as defined in equations~\ref{eq:risk} and~\ref{eq:min_f}, and the test set is used to evaluate the generalization error since it is unseen data for the model. In practice, we choose a percentage of data (e.g.\ {20\%}) that defines the size of the test set from an initial dataset, and the randomness of the split ensures that the data from both sets are similar yet not exactly the same. However, in some cases, one should be aware that outliers can be present in the test set, which makes the performance on the test set worse than expected. Or in some cases, the dataset is too small and every data point are very different from each other, and the test set is very different from the training set, which makes it impossible for the model to predict on the test with the parcelar information given by the training set. The percentage of the train/test split should therefore be chosen wisely and according to the dataset so that it remains representative of the training set.

The main property of the test set is that it is a completely unsee dataset, which means that the training of model should be independent of this set except for the very final evaluation. But in some cases, we want to compare different models with each other or change a ``hyperparameter'' like the regularization parameter within the same model architecture. To evaluate theses models, we cannot evaluate for every model the generalization error on the test set, because it would compromise the independence of the test set with the training process. Hence, we introduce validation sets within the initial training set. We could do a simple training/validation split like we did for the test set, however it would weaken the model training even more since there is less data available, and furthermore it does not use all the potential of training set. A very common technique to test the performance of a model on a training set is the cross-validation. The idea is to use different training/validation splits to test the model in multiple configurations to have a better evaluation of the model performance by averaging the different performances. 

The most used method is the k-fold cross-validation technique which consists in partioning the training set $\mathcal{D}\e{training}$ in k equal size subsets $\mathcal{S}_1,...,\mathcal{S}_k$. The model is then trained on the union $\bigcup_{l\neq m} \mathcal{S}_l$ of all subsets but one subset $\mathcal{S}_m$ that will be used as a validation set for all $m\in \{1,\ldots,k\}$. The principle of the k-fold approach is illustrated on the Figure~\ref{fgr:kfold}. The approximate generalization error of the model is then the average of every loss calculated on the validation subsets. This tool provides a way of comparing different models without using the test set, which is extremely useful especially in the parameterization of the ML model.

\begin{figure}[ht]
  \centering
    \includegraphics[width=0.6\textwidth]{figures/4-ml/1280px-K-fold_cross_validation_EN.svg.png}
    \caption{A k-folds cross-validation illustration adapted from \href{https://en.wikipedia.org/wiki/File:1280px-K-fold_cross_validation_EN.svg.png}{Wikimedia Commons} under a \href{https://creativecommons.org/licenses/by-sa/4.0}{CC BY-SA 4.0} license. \todo{redo if time}}\label{fgr:kfold}
\end{figure}

Other cross-validation techniques exist and are used in specific cases, for example the stratification corss-validation consists in ensuring the same repartition of the labels $y_i$ in each subset, which is useful in classification problems. We can also make the validaiton process even more exhaustive by increasing the k in the k-fold validation, however it requires k training of the models, which increase the computation time required. When increase to the maximum k is equal to the size of the training set and the method is called leave-one-out cross-validation. Finally, for time series, the cross-validation technique usually requires to sort the data according to the time history so that the training set is always prior to the validation set, which creates a whole new approach to cross-validation. The core idea of cross-validation is to find multiple training/validation splits to evaluate the model on more than one angle, and different strategies exist depending on the training problem before us.

\subsection{Machine learning models}\label{sct:model}

In this chapter, we will use the eXtreme Gradient Boosting (XGBoost) as the machine learning framework for our predictive model because of its accuracy, efficiency and simplicity of use. Its performance has long been proven since 17 out 29 Kaggle challenge winning solutions were based on this algorithm in 2015. The XGBoost system is highly scalable and parallelized, which gives very fast model training.\autocite{chen2016xgboost} Compared to more standard tree-based algorithms such as random forest (commonly used in the field~\autocite{Simon_2015}), the boosting component of the algorithm means that it learns from previous mistakes and puts higher weights on the problematic data points, hence improving the accuracy of the final ML model. In this section, we will go from the basic components of the model (decision tree) to the more complex ensemble model (e.g. random forest), and to finish with the gradient boosting model. The discussion will be mainly focused on regression problems and not classification problems since the goal is to predict a continuous variable (the xenon/krypton selectivity).

\subsubsection{Regression tree}

Tree-based models are usually used in classification problem where depending on a set of ``yes'' or ``no'' questions the tree classifies the data points into the different predfined categories. The questions are in fact associated to threshold values of the $p$ features or characteristics $C_1,\ldots,C_p$; for example a not of the tree could ask the question ``Is $C_1$ higher than $3$?'', which splits the space into two categories the ``yes'' and the ``no''. This is why we can see a decision tree model as a splitting of the space into rectangles (in 2D) or an equivalent of a rectangle in $p$-dimensional feature space. To adapt this type of model into a regression problem, we can group different label values $y$ into categories that are represented by the average label values. To sum up, a decision tree for regression splits the feature space into a set of pseudo-rectangles (volumes separated by limited hyper-surfaces) defined by the nodes of the tree, and in each of these subspaces are given the average of the different points present in this subspace. To clarify the terminology, a splitting node correspond to a separation between regions, while a terminal node or leaf corresponds to the region itself.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.45\textwidth]{figures/4-ml/tree_example.png}
  \includegraphics[width=0.45\textwidth]{figures/4-ml/tree_regions_example.png}
  \caption{Illustration of the decision tree and the region splitting performed by a CART\autocite{Breiman_2017} algorithm, taken from the book ``Elements of Statistical Learning'' \cite{Hastie_2009}. \todo{refaire le graphe ?}}\label{fgr:tree}
\end{figure}

The CART\autocite{Breiman_2017} algorithm developed by Breiman et al. is usually presented as the archetype of a decision tree model. The algorithm is pretty straightforward to understand, three steps are required: (1) Examine every split allowed on each feature $C_i$, (2) select and use the best split according to a loss function (squared error or absolute error usually), and (3) stop splitting a node when a stopping rule is satisfied (e.g. mininimum samples split).\autocite{Dension_1998} We could split indefinitely the decision tree so that each data point has its own region, but this would be a textbook case of overfitting, any new datapoint would never be correctly predicted with such a model. To prevent this from happening, the decision tree has a regularization parameter called mininimum samples split $n\e{min}$ that only allows a further split; if the node contains less than a given number $n\e{min}$, then it is necessarily a terminal node. The decision trees are known to be very prone to overfitting, another useful regularization parameter can be used to prevent an evergrowing tree is the maximum depth of the tree,  which can also stop the iterative process of tree growing. Finally, to further regularize the tree, a process called tree pruning simplifies the tree and outputs the final model. We won't go into the details of tree pruning as it is not the main subject (see Ref.~\cite{Hastie_2009} for further details). The final tree $f\e{tree}$ can be expressed as a function of the different regions $R_1,\ldots,R_M$ carved by the splitting process:
\begin{equation}
  f\e{tree}(\mathbf{x}) = \sum_{m=1}^M c_m \mathbb{1}(\mathbf{x}\in R_m)
\end{equation}
where $c_m$ is the value of the leaf corresponding to $R_m$, and $\mathbb{1}$ is a identity function that returns $1$ if the argument is true and $0$ otherwise. The coefficients $c_m$ of this fonction are actually equal to the average of the labeling values of the dataset $\mathcal{D}_n$ in the region $R_m$, i.e. $c_m= \underset{i\in \mathcal{D}_n}{\text{ave}}(y_i|\mathbf{x}_i\in R_m)$.
To put it simply, the tree function returns the average value of $y$ (in the dataset) in the region where $x$ (could be new data) is located. 

The decision tree has the main advantage of being very easily interpretable as defined in the book of C. Molnar \cite{molnar2020interpretable}. This interpretability boils down to the easily understandable binary decision at the root of the decision tree --- we can easily see the explaining characteristics ($R_m$) of a predicted value, we can easily imagine different predictions depending on the value of $\mathbf{x}$, and for small trees we can even run the model in our own head. However, this model has a reputation of being very inefficient in finding simple linear relations resulting in a step-like function. The model is not very smooth, slight changes in the input $x$ can have a big impact on the predicted value (typically near the sepration between two regions) and some change (noise) in the training data can totally change the structure of the tree. This unstability of a single tree makes it very hard to generalize over unseen data.\autocite{molnar2020interpretable} To improve this single decision tree, Breiman introduced bagging predictors in 1996 to improve the accuracy of models that are unstable with regard to small changes in the learning set.\autocite{Breiman_1996} This new approach is at the origin of the random forest, and will be presented more in depth in the following subsection.

\subsubsection{Random Forest}

The core idea behind random forest is that an collection of weak learners, called an ensemble model, is better than a single strong learner, this assumption relies on a proven theorem that states that the minimal error of a forest is lower than the error of a single tree (theorem 11.2. of Ref.~\cite{Breiman_2001}). The strength of model depends on the amount of information we feed into the model and its complexity. To achieve a diverse forest of weaker decision trees, we need to introduce two concepts; the first one is the boostrap aggregating (bagging) and second one is the random column subsampling. Both methods ensure a diversity in the generated trees by using random selections and also a relative weakness of the trees by reducing the amount of information it can access. 

The bagging method consists in generating a set $\left\{\phi_b\right\}_{b\in \{1,\ldots,B\}}$ of $B$ models from different bootstrap datasets $\left\{\mathcal{D}\ex{train}_b\right\}_{b\in \{1,\ldots,B\}}$. Each boostrap dataset $\mathcal{D}\ex{train}_b$ is generated by randomly selecting $t$ elements of $\mathcal{D}\ex{train}$ using a sample with replacement --- note that each bootstrap sample has the same number of elements than $\mathcal{D}\ex{train}$ but data points can appear several times in it. The number of times a data point $(\mathbf{x}_i,y_i)$ appears represents the weight of this data point in the bootstrap learning set. To simplify, we can say that each tree model $\phi_b$ learns on the $\mathcal{D}\ex{train}_b$ dataset that has randomly defined weights on the different data points, which means that every model will pay attention to different parts of the training data. We can also evaluate the generalization error of the model since some trees have never seen some data points, we can evaluate the generalization error on the unseen data for every tree (similar to cross-validation), this error is called the out-of-bag error.

The second technique consists in randomly choosing a subsample of the features on which to find the best split (second part of the CART tree growing algorithm). This technique is inspired from the one developed by Ho in 1998, where each tree of a forest is trained only on a randomly chosen feature subspace.\autocite{Tin_Kam_Ho_1998} The only tweak in the procedure lies in the fact that the feature space changes at each iteration of the tree growing process instead of between each tree generation. This method also improves the generalizability of the method by weakening each tree so that they don't overfit, the accuracy is achieved by the aggregation of all the trees. 

The random forest as formulated by Breiman combines these two randomness-based techniques to train a forest.\autocite{Breiman_2001} The algorithm starts by looping over the number of trees $B$ in the forest, for each tree $b$ a bootstrap sample $\mathcal{D}\ex{train}_b$ is randomly drawn (with replacement) and this data is used to grow the tree (training procedure). In the training, a modified CART algorithm is applied to grow the tree by splitting recursively on each node: (1) instead of testing all features for the best splitting, only a random selection of $m$ variables is considered among the $p$ features, (2) the best split point is selected among the $m$ variables, and (3) the node is split in two until the minimum leaf size $n\e{min}$ is reached. The size of the column subsample defines the number of features to randomly consider at each split; this is another implicit regularization parameter associated with the random forest along with the previously identifies regularization parameters of the decision tree such as the minimal leaf size $n\e{min}$ or the maximal depth of a tree. Finally we have a set of $B$ trees $\left\{\phi_b\right\}$ that can be used to make an ensemble model $\phi$ so that:
\begin{equation}
  \phi(\mathbf{x}) = \frac{1}{B}\sum_{b=1}^{B} \phi_b(\mathbf{x}) = \frac{1}{B}\sum_{b=1}^{B}\sum_{m=1}^{M_b} c_{m,b} \mathbb{1}(\mathbf{x}\in R_{m,b})
\end{equation}

\subsubsection{Gradient Boosting}

Gradient descent / gradient boost
\todo{compare to the optimization of molecular structure in an \emph{ab intio} simulation. Find a minimum, gradient descent}

Ada boost

learning rate, 

\subsubsection{XGBoost}


\todo{compile all relevant hyperparameters}
%learning rate, colsample_bytree, colsample_bylevel

\section{Prediction of the Ambient-pressure Selectivity}

Simon et al. published one of the first articles on an ML-assisted screening approach for the separation of a Xe/Kr mixture extracted from the atmosphere.\autocite{Simon_2015} Their model's performance was highly relying on the Voronoi energy, which is basically an average of the interaction energies of a xenon atom at each Voronoi node.\autocite{Rycroft_2009} To rationalize this increase in performance, we regarded this Voronoi energy as a faster proxy for the adsorption enthalpy. By comparing it to the standard Widom insertion, we found that although it is faster, it is less accurate; and we developed a more effective alternative, the surface sampling (RAESS) using symmetry and non-accessible volumes blocking.\autocite{Ren_2023} Recently, Shi et al. used an energy grid to generate energy histograms as a descriptor for their ML model, which gives an exhaustive description of the infinitely diluted adsorption energies,\autocite{Shi_2023} but can be computationally expensive.

All the approaches described above can have good accuracy in the prediction of low-pressure adsorption (i.e., in the limit of zero loading) but are not suitable for prediction of adsorption in the high-pressure regime, when the material is near saturation uptake. While this later task is routinely performed by Grand Canonical Monte Carlo (GCMC) simulations, there is a lack of methods at lower computational cost for high-throughput screening. To better frame our challenge, in this work we are essentially trying to predict the selectivity in the nanopores of a material at high pressure, where adsorbates are interacting with each other, while only having information on the interaction at infinite dilution. The comparison between the low and high-pressure cases gives key information on the origin of the differences of selectivity. For instance, we have previously shown that selectivity could drop between the low and ambient pressure cases in the Xe/Kr separation application, and it was mainly attributed to the presence of different pore sizes and potential reorganizations due to adsorbate--adsorbate interactions.\autocite{Ren_2021}

In this section, we combined a grid-based approach with core components of our previously developed RAESS algorithm~\autocite{Ren_2023} to design a new adsorption energy sampling technique. Moreover, a statistical characterization of the pore size and energy distributions has been performed to inform the model on a potential selectivity drop. By combining these two approaches, we propose a set of useful ML descriptors for fast and accurate ambient-pressure selectivity prediction, and we highlight its performance on the case of xenon/krypton separation in the CoRE MOF 2019 database\autocite{Chung_2019}.\todo{ref previous chapter}



\subsection{Model training}

\subsubsection{The machine learning model}

We chose to use eXtreme Gradient Boosting (XGBoost) as the machine learning framework for our predictive model because of its accuracy, efficiency and simplicity of use. Its performance has long been proven since 17 out 29 Kaggle challenge winning solutions were based on this algorithm in 2015. The XGBoost system is highly scalable and parallelized, which gives very fast model training.\autocite{chen2016xgboost} Compared to more standard tree-based algorithms such as random forest (commonly used in the field~\autocite{Simon_2015}), the boosting component of the algorithm means that it learns from previous mistakes and puts higher weights on the problematic data points, hence improving the accuracy of the final ML model.

In the next sections, we introduce new descriptors for nanoporous materials, as well as new concepts of feature engineering based on energy and pore size histograms. The ML features presented have been selected by progressively filtering out the less influential ones on the accuracy of the final model, see the complete list in Table~S1-3 of Supporting Information (SI). The influence or importance is defined later in a section dedicated to the interpretation of the model. The hyperparameters of the model were fine-tuned using random searches to design the best performing final model. Finally, the influence of the preselected descriptors on the final model is interpreted using a unified approach.

\subsubsection{Target variable}

We want to predict the Xe/Kr ambient-pressure selectivity faster than standard techniques. To obtain reference values (ground truth), we used the Raspa2 software\autocite{dubbeldam2016} to run grand canonical Monte Carlo (GCMC) calculations of 20-80 Xe/Kr mixtures at \SI{298}{\kelvin} and \SI{1}{\atm} on our cleaned database. The van der Waals interactions are described by a Lennard-Jones (LJ) potential with a cutoff distance of \SI{12}{\angstrom}. The LJ parameters of the framework atoms are given by the universal force field (UFF),\autocite{rappe1992} and the guest atoms (xenon and krypton) have their LJ parameters taken from a previous screening study.\autocite{Ryan_2010} The study only focuses on a given Xe/Kr composition usually obtained by cryogenic distillation of ambient air~\autocite{kerry2007industrial} as a first step towards predicting other mixtures at different physical conditions (\emph{e.g.} Xe/Kr mixtures out of nuclear off-gases). In the broader scope, this methodology could be adapted to the desired application with some tweaks on the descriptors calculation (\emph{e.g.} \ce{CO2}/\ce{CH4} separation).

We decided to use a logarithmic transform of the selectivity instead of the raw value because we are more interested in the order of magnitude of the selectivity values than to predict the higher values of selectivity --- an ML model that predicts selectivity values can lower down the errors by focusing the prediction more on the higher values than the lower ones. By focusing on the logarithmic transform of the selectivity, we can better separate the different orders of magnitude of the selectivity values. This approach distributes more evenly the efforts on all the different values of selectivity. Moreover, this logarithmic transform is related to a thermodynamic quantity that we elaborate later in the section~\ref{sct:thermo}; it can therefore be easily compared with the energy descriptors we introduced in this article.

\subsubsection{Database and data preparation}

To test our methodology on a set of realistic MOFs, we chose to screen the 12,020 all-solvent removed (ASR) structures of the CoRE MOF 2019 database\autocite{Chung_2019}. After removing the disordered and the non-MOF structures as well as the ones with a large unit cell volume of \SI{20}{\cubic\nano\meter}, we obtained a set of 9,748 structures. Then we analyze the string information given by the Zeo++ software\autocite{zeopp_Willems2012} to reduce the number to 9,177 by removing the structures that are not tridimensional, where solvents are still detected (wrongly classified in ``all solvent removed), or where the metal is radioactive or fissile (e.g., Pu-MOF TAGCIP\autocite{Diwu_2010}, Np-MOF KASHUK\autocite{Martin_2017}, U-MOF ABETAE\autocite{Jouffret_2011} or Th-MOF ASAMUE\autocite{Liang_2016}) --- this can be a source of risks in a nuclear waste processing plant. Furthermore, we added a condition on the largest cavity diameter (LCD) to keep only the structures that can accept a xenon molecule: 8,529 structures have an LCD higher than \SI{4}{\angstrom} (approximately the size of a xenon molecule). This is equivalent to removing the structures with very unfavorable adsorption enthalpies, that are not promising for our adsorption-based separation (see previous work~\autocite{Ren_2023}).

Then, the descriptors summarized below (and fully detailed in Supporting Information) were calculated on this restrained dataset. At this stage, 370 structures failed to be calculated in GCMC and 82 have no standard deviation for the pore distribution (skewness and kurtosis cannot be retrieved). A final dataset of 8,077 structures was therefore used to perform our ML-assisted method of screening the Xe/Kr adsorption selectivity. Based on this final set, {20\%} were randomly used for the test set and {80\%} were used to train our model. The goal is to learn from the training set a relationship between the descriptors and the target ambient-pressure selectivity in order to evaluate the performance on the test set. A CSV file of training and test sets can be found in the data availability section.

\subsubsection{Geometrical and chemical ML descriptors}

Looking at a number of different research papers on supervised ML for the prediction of adsorption properties,\autocite{Fernandez_2013,Simon_2015,Fanourgakis_2020,Anderson_2020,Pardakhti_2020} we see that some descriptors are recurrent: 1) geometrical descriptors obtained by software like Zeo++~\autocite{zeopp_Willems2012} such as the surface area (SA), the void fraction (VF), the largest cavity diameter (LCD) and the pore limiting diameter (PLD); and 2) physical and chemical descriptors such as the framework's density, the framework's molar mass, the percentage of carbon (C\%), nitrogen (N\%), oxygen (O\%), hydrogen but also halogen, nonmetals, metalloids and metals, and the degree of unsaturation. Although these descriptors are very versatile and used in many ML models, they, however, fail to provide specific information for our ML task. As shown by Simon et al., energy descriptors are greatly influential in ML models for selectivity prediction.

The geometric analysis of the crystalline porous materials is typically based on the van der Waals (vdW) radii predefined by the Cambridge Crystallographic Data Centre (CCDC). This force field-independent choice can create a gap between the geometrical descriptors and the thermodynamic values obtained through molecular simulations. Inspired by a recent work on the comparison of PLDs and self-diffusion coefficients,\autocite{Hung_2021} we defined a list of vdW radii to be read by the Zeo++ software (more details in \url{https://github.com/eren125/zeopp_radtable}). In this study, all Zeo++ calculations use an atomic radius that corresponds to the distance where the LJ potential reaches $3 k_\text{B} T/2$, for $T = \SI{298}{\kelvin}$.

The SA exposed to different probe sizes (\SI{1.2}{\angstrom}, \SI{1.8}{\angstrom} and \SI{2.0}{\angstrom}) were tested. The probe occupiable volume was chosen to measure the void fraction (VF) for different adsorbent by using probe sizes of \SI{1.8}{\angstrom} (close to the radius of krypton) and \SI{2.0}{\angstrom} (close to that of xenon). This definition of the pore volume was found to be in better agreement with experimental nitrogen isotherms.\autocite{vol_Ongari2017}

Because our goal is to predict the difference between the low-pressure selectivity and the ambient-pressure one (for a given gas mixture composition), some of these descriptors have very little importance, and the key factor is the difference of accessible volume and the affinity of the remaining pore volume with xenon, compared to krypton. The intuition developed in the previous study sketched the role of a diverse distribution of pores with different xenon affinities.\autocite{Ren_2021} For all these reasons, from all the ``standard'' descriptors taken from the literature, we kept only the following 7 descriptors: C\%, N\%, O\%, LCD ("D\_i\_vdw\_uff298"), PLD ("D\_f\_vdw\_uff298"), SA for a \SI{1.2}{\angstrom} probe ("ASA\_m2/cm3\_1.2") and VF for a \SI{2.0}{\angstrom} probe ("PO\_VF\_2.0"). We also built a new descriptor $\Delta \text{VF}$ void fraction values, the difference of volumes occupiable by xenon (\SI{2.0}{\angstrom}) and by krypton (\SI{1.8}{\angstrom}). All these descriptors along with other pore size distribution based geometrical descriptors are presented in detail in the Table~S1 of the Supplementary Information (SI).

\subsection{Pore size distribution}

To generate a histogram of pore sizes (or pore size distribution, PSD), Monte Carlo steps are used to measure the frequency of every accessible pore sizes binned by \SI{0.1}{\angstrom}.\autocite{poresize_Pinheiro2013} This histogram can then be used to generate descriptors based on statistical parameters that describes the overall location, the dispersion, the shape and the modality of the distribution. In addition to the mean and standard deviation of the distribution, we introduced two additional moments: the skewness ($\gamma$) corresponds to the third standardized moment and measures the asymmetry of a distribution; and the kurtosis ($k$), being the fourth standardized moment, measures the relative weight of the tails of the distribution. Knowing the importance of characterizing the number of different pore sizes suspected to be at the origin of the selectivity drop observed, we tried to find a simple descriptor to measure the number of modes in the distribution. The Sarle's bimodality coefficient, BC $= (\gamma^2 +1)/k$, represents a simple quantification of how far we are from the unimodality based only on skewness and kurtosis.\autocite{Tarba_2022}
Finally, to further measure the diversity of pores, we introduced an effective number $n\e{eff} = N^2/\sum n_i^2$ of pore sizes, where $N$ is the total number of points in the histogram and $n_i$ the number of points associated with the $i^\text{th}$ bin. This number is very similar to a statistical number widely used in other scientific fields: in political science it is used to measure the effective number of political parties,~\autocite{neffposci_Laakso1979}, in ecology the inverse Simpson's index evaluates the species diversity in an ecosystem,\autocite{neffbio_Simpson1949} or in quantum physics the inverse participation number measures the degree of localization of a wave-function.\autocite{neffphys_Kramer1993} This effective number of pore sizes gives an idea of the diversity of pore sizes (considering a binning of \SI{0.1}{\angstrom}). A highly effective number would mean that multiple pore sizes are highly represented in the structure; this descriptor gives an idea of how scattered the pore sizes are.
All these descriptors carry information on the form of the PSD needed to figure out the loading and selectivity situation in the framework near saturation uptake, which is crucial to predict the ambient-pressure selectivity.

\subsubsection{From infinite dilution to ambient pressure}

The low-pressure selectivity provides a first intuition of the selectivity at higher pressure, as demonstrated in our previous work showing a correlation between the selectivity at both pressures.\autocite{Ren_2021} If we adopt the Gibbs free energy formalism (Equation~\ref{eq:exc_gibbs_free_energy}), which correspond to a logarithmic transform of the selectivity values, this correlation is confirmed and highlighted on Figure~\ref{fgr:problem}. We can also note that although a majority of structures have similar selectivity in both pressure conditions, a handful of structures experience a selectivity drop at higher pressure. The zero-loading selectivity is always higher or similar to the ambient-pressure one, it gives therefore a solid ground on which to build an efficient prediction model. The second ingredient for a good prediction model is to build explanatory descriptors related to this selectivity drop phenomenon. One of the main causes to the selectivity drop being the presence of bigger pores that are less attractive xenon, therefore additional information on the pore size distributions or the energy landscape would be helpful for this task.

\begin{figure}[ht]
\centering
  \includegraphics[width=0.5\linewidth]{figures/4-ml/main/Scatterplot_G1_G0.pdf}
  \caption{Comparison between the Gibbs free energy of exchange at low pressure $\Delta G\e{0}$ and ambient pressure $\Delta G\e{1}$ labeled by the relative distance between them. This plot is equivalent to a logarithmic plot of the selectivity at these two pressure conditions.}
  \label{fgr:problem}
\end{figure}

%% Pore size distribution & statistical descriptors
To incorporate information on the pore size diversity of the materials, we carried out statistical measurements on the PSD. By analyzing them, we detected explanatory factors at the origin of the observed selectivity drop. A high degree of multi-modality in the distribution would mean a diverse set of pores, which can lead to a selectivity drop if the pores are significantly different one from another. The more distant is the average pore size from the largest cavity diameter the higher the chance of observing a selectivity drop, because a big difference between the pore sizes bring about a lower selectivity. All these statistics are designed to give as much knowledge as possible on a hypothetical selectivity drop and on the quantitative estimation of its magnitude.

\todo{refer to previous chapter}
%% Energy descriptors statistics
To better quantify the change of selectivity, it could be interesting to give statistics on the distribution of interaction energies for xenon and krypton calculated by our grid algorithm. These statistics include moments of different orders (up to 4) of the energy distribution, which informs on the adsorbate--adsorbent interaction energies in the nanopores at higher loading. The shape of the energy distribution can help assess quantitatively the change in selectivity. We can consider this as a way of compressing the whole energy distribution into a few statistical values, which is a standard method used in the field of data science to tackle distribution data. The same approach has also been applied to the Boltzmann weighted distributions to generate temperature specific descriptors for the energy distributions.

\todo{refer to previous chapter, change text cuz copy paste}
%% 900K gaps
By using different temperatures, we noted that the infinite dilution adsorption enthalpies at higher temperatures can be better correlated to the adsorption enthalpy at ambient pressure. The minimum error was found for the adsorption enthalpy at \SI{900}{\kelvin}, which gives an RMSE of \SI{1.76}{\kilo\joule\per\mole} instead of \SI{2.87}{\kilo\joule\per\mole} for the \SI{298}{\kelvin} case. This new type of descriptor is very interesting since it better performs around the high selectivity region, where the standard Boltzmann average at \SI{298}{\kelvin} loses its accuracy (see Figure~S1). As we can see in the Figure~S7, the exchange free energy at \SI{900}{\kelvin} and the excess of free energy compared to the \SI{298}{\kelvin} case are the second and third most influential descriptors of our ML model. They are complementary to the exchange free energy at \SI{298}{\kelvin} to predict selectivities at higher pressures.

%% conclusion
By combining the above-mentioned features with more standard geometrical descriptors, we trained an ML model for the ambient pressure selectivity that identifies the origins of the selectivity drop and gives promising prediction results.

\subsubsection{Hyperparameter optimization}

\begin{lstlisting}[language=Python]
  optimal_params = {
      'objective':'reg:squarederror',
      'n_estimators': 1500,
      'max_depth': 6,
      'colsample_bytree':0.85,
      'colsample_bylevel':0.65,
      'subsample':0.7,
      'alpha': 0.4,
      'lambda':0,
      'gamma' :0,
      'learning_rate': 0.04,
      }
  \end{lstlisting}

\todo{add SI data on hyperparameter}
We used the training data to perform a random search of hyperparameters, with 5-fold cross-validation to evaluate the root mean squared errors (RMSE) of the model. The range of search explored for each hyperparameter is made available in the SI. After this search, a set of optimal hyperparameters were identified that give an average RMSE of \SI{0.36}{\kilo\joule\per\mole}; we used it to build our final model. A convergence plot of the model performed using 5-fold cross-validations is given in Figure~S6. Given this configuration, the model is tested on the prior defined test-set and interpretation tools are used to better understand the structure-property relationships in play.

\subsection{ML model performance}

%% introduction
In this section, we present the performance of the ML model that learned the joint effects of all the newly introduced descriptors to detect and evaluate the observed drop between the easily accessible low-pressure selectivity and the more computationally demanding ambient-pressure selectivity.
A GCMC simulation of a 20-80 xenon/krypton gas mixture takes in average \SI{2,400}{\second} per structure on the CoRE MOF 2019 database, while our grid-based adsorption calculation only takes about \SI{5}{\second} per structure (on a single Intel Xeon Platinum 8168 core at \SI{2.7}{\giga\hertz}). To compute all features needed for our prediction, we would need less than a minute per structure, which is way faster than the 40 minutes required for a GCMC calculation. The ML-based approach has a very clear speed advantage over standard molecular simulations. But to be a good substitute, it needs to keep a good level of accuracy on an unseen set of structures.

\begin{figure}[ht]
\centering
  \includegraphics[width=0.5\linewidth]{figures/4-ml/main/Scatterplot_G1_prediction.pdf}
  \caption{Scatter plot of the exchange free energy predicted by the model. There is a good agreement between the predicted and true values. On the test set, there is an RMSE of \SI{0.37}{\kilo\joule\per\mole} and an MAE of \SI{0.21}{\kilo\joule\per\mole}. This plot is equivalent to the scatter plot between the logarithm of the ambient-pressure selectivities (see Figure~S5 of the SI). The corresponding errors for the ambient-pressure selectivity are 2.5 and 1.1 for respectively the RMSE and MAE of the selectivity, and 0.065 and 0.038 for the RMSE and MAE of its base-10 logarithm. }
  \label{fgr:G1_prediction}
\end{figure}

We defined a set of {80\%} randomly chosen structures out of the final dataset to train and fine-tune the parameters of our model. A randomized search over a range of maximum depths, learning rates, sizes of feature samples used by tree and by level, sizes of data sample and alpha regularization parameters has been performed and a set of hyperparameters have been chosen to minimize the average RMSE computed using a 5-fold cross-validation. The ranges used in the randomized search as well as the final hyperparameters set are given in SI. By using this parameterization, our XGBoost model has an RMSE of \SI{0.37}{\kilo\joule\per\mole} and an MAE of \SI{0.21}{\kilo\joule\per\mole} on the exchange Gibbs free energies of the test set of 1,616 structures. If we convert back these results to the selectivity values, the RMSE on the selectivity values would be 2.5 and 0.07 on the logarithm base 10 of the selectivity, which means that the order of magnitude of the selectivity is known with a very good accuracy. To prove that this good performance is not fortuitous, we used a 5-fold cross-validation procedure on the whole dataset and found an average RMSE of \SI{0.36}{\kilo\joule\per\mole} with a standard deviation of \SI{0.01}{\kilo\joule\per\mole}, which is consistent with the performance given by a standard train/test split.

This method can later be used in a screening procedure that relies on cheap descriptors to skim off obviously undesirable structures to only keep the promising structures for the final ML model evaluation. For this is the reason, as previously explained in the methods, only the 3D MOF structures with an LCD above \SI{4}{\angstrom} are kept because they have a positive xenon affinity, which is a necessary condition for a good Xe/Kr selectivity. Our model being very good at predicting the ambient pressure selectivity of structures with good xenon affinity, the proposed screening procedure, illustrated Figure~\ref{fgr:pipeline}, would include (i) a check of the nature of the structure to ensure it is a 3D MOF structure, (ii) then a filter on the LCD value (above \SI{4}{\angstrom}), (iii) a pre-evaluation of the Xe/Kr selectivity at infinite dilution using the grid-based method, and (iv) finally the ML evaluation to keep only structures above a certain threshold of ambient-pressure selectivity (\emph{e.g.} 30). We could eventually evaluate more thoroughly the top structures using GCMC simulations, \emph{ab initio} calculations or adsorption experiments.

\begin{figure}[ht]
\centering
  \includegraphics[width=0.99\linewidth]{figures/4-ml/main/pipeline.pdf}
  \caption{An illustration of the screening procedure that could be used to find highly selective materials.}
  \label{fgr:pipeline}
\end{figure}

\section{Opening the Black Box}

%% SHAP intro
To better understand the intuition behind this selectivity drop, we used the SHAP\autocite{SHAP,molnar2020interpretable} library of interpretation models to draw relationships between the descriptors and the predicted ambient-pressure selectivity. This code library is based on the calculation of Shapley values\autocite{shapley1953value} that measure the contribution of each descriptor to the prediction to locally interpret our ML model. This interpretation model untangles the interdependence between the descriptors to extract an individual contribution. To go beyond the local interpretation, we can rapidly compute the Shapley values for the whole dataset using faster algorithms;\autocite{SHAP} scatter plots of the contribution as a function of the descriptor values called SHAP dependence plots can then be drawn to make a more global interpretation of our ML model. Knowing a descriptor value, we could then infer, with a certain level of uncertainty, how it changes the final predicted value, which highlights unknown structure--property relationships. Finally, we can use the mean absolute Shapley values of each feature on the training set to measure the feature importance (see Figure~S7 and S8).

\subsubsection{Explainable AI}

\todo{Present the explainable AI}
The final model is trained on the predefined training set using XGBoost with the fine-tuned hyperparameters. By testing it on the test set, we measure the accuracy of our approach, however, it is interesting to extract chemical insight into the hidden relationship between the predicted value and the descriptors, to better understand the thermodynamic origins of the performance. In this work, we used the Shapley values,\autocite{shapley1953value} a game theory concept developed by Shapley in 1953, to measure the contribution of each descriptor in the predicted value. This tool is used locally to understand for a given structure how their characteristics had contributed to the prediction. To draw structure-property relationships, we would need to use a global interpretation methods such as the SHapley Additive exPlanations (SHAP) method thoroughly detailed in the online book \emph{Interpretable Machine Learning} of Christoph Molnar.\autocite{molnar2020interpretable} The SHAP tool developed by Lundberg and Lee~\autocite{SHAP} is based on a faster algorithm adapted to tree-based ML models like gradient boosting, TreeSHAP, and integrates useful global interpretation modules like SHAP feature importance and dependence plot.

\subsection{Global interpretability}

To rank the descriptors according to their average impact on the magnitude of the model output, we can use the mean absolute Shapley values of each descriptor. The importance plot associated with these values are presented in Figure~S8. Even if the low-selectivity exchange Gibbs free energy has a SHAP importance value way above the others, it only serves as a baseline where a correlation close to the one presented on Figure~\ref{fgr:problem} can be reached; the other descriptors play a major role in moving the outliers of the figure closer to the diagonal line. Energy descriptors play a major role in the model's prediction, and the geometry-based new descriptors, while playing a more secondary role, are key in evaluating the gaps between the low-pressure case with the ambient-pressure one that we are interested in. To dig deeper into the mechanisms that allow the model to predict the selectivity with a very good accuracy --- the RMSE and MAE on the test set's selectivity being respectively $2.5$ and $1.1$ --- we are now going to look into the SHAP dependence plots of each interesting descriptor that plots the contribution to the predicted value as a function of the actual descriptor value.

%% Global Interpretability dependence plot
To make a global interpretation, we applied the partial dependence module provided by the SHAP library on our model. Although other methods to compute dependence plots exist (\emph{e.g.} partial dependence plots),\autocite{molnar2020interpretable} we can keep a good level of consistency between our global and local interpretations by using the same underlying theory. The SHAP dependence plots of all the descriptors of the Figures~S9 and S10, these plots have a rather distinct form, directions and shape, which is encouraging for the interpretability of our model. By looking at the profile of the dependence plots, we can extract valuable information on how the ML model predicts the ambient-pressure selectivity.

%% strong relations
The most important descriptor is obviously the exchange free energy "G\_0" associated to the low-pressure selectivity, its contribution has a very strong positive linear correlation (see Figure~\ref{fgr:pdp_selection}), which gives a base value on top of which the other contributions will either reduce the free energy (more selective) or increase it (less selective). The model can be interpreted as the combination of a baseline combined with smaller tweaks that estimate the magnitude of the deviation from the ideal low dilution case. For instance, the next two descriptors "G\_900K" (\SI{900}{\kelvin} low-pressure exchange free energy) and "G\_Xe\_900K" (\SI{900}{\kelvin} low-pressure xenon adsorption free energy) continue to build up the baseline by providing information on the low-pressure selectivity, but they start giving a glimpse of deviations needed to differentiate between the structures experiencing a drop with the ones that keep their selectivity. As we can see in the SI (Figure~S1 and S2), the thermodynamic quantities at high pressure is closer to the \SI{900}{\kelvin} case than to the ambient temperature one, these two descriptors inform naturally on the selectivity at higher pressure. For "G\_900K" (see Figure~\ref{fgr:pdp_selection}), blue points (corresponding to a "G\_0" of around \SI{-8}{\kilo\joule\per\mole}) can have either negative or negligible contributions depending on the value; values below \SI{-4}{\kilo\joule\per\mole} give a negative contribution with a linear relation, whereas values between $-4$ and \SI{5}{\kilo\joule\per\mole} give constantly almost zero contributions. This type of domain differentiation illustrates how the model can identify structures with a selectivity drop based on the values of a descriptor. We will see more telling examples of how the contribution to the selectivity values are determined using the values of the remaining descriptors.

%% optimal values
The U-shape of some SHAP dependence plots can highlight optimal values for the associated descriptors. For instance, the optimal value of "D\_i\_vdw\_uff298" is around $5.1$ (see Figure~\ref{fgr:pdp_selection}) and the optimal average of pore sizes is around $5.6$. These optimal values match with the physical need of having pores of the size of a xenon to be more attractive to it, which was identified in several papers in the literature. We can note that these values are a bit higher than the ones mentioned in the literature due to the different definition of the atom radii.\autocite{Hung_2021} Moreover, values of "delta\_G0\_298\_900" between $4$ and \SI{6}{\kilo\joule\per\mole} (see Figure~\ref{fgr:pdp_selection}) have a higher chance of giving a negative contribution, which means a lower ambient-pressure selectivity. These sweet spots constitute valuable hints to tell the truly selective materials from the others. Some SHAP dependence plots have a rather linear domain for the most selective structures (in blue) --- the difference of pore volumes between Xe and Kr sized probes "delta\_VF\_18\_20" have a good linear contribution (see Figure~\ref{fgr:pdp_selection}), which means that the lower the more selective the structure will be. The same can be said for the standard deviations of the PSD "pore\_dist\_std" and of the Boltzmann weighted krypton interaction energies distribution "enthalpy\_std\_krypton". The optimal values for these descriptors are zero, the closest to zero it is the more negative the contribution will be and the more selective the structure at ambient pressure.

%% optimal domains (weak relations)
Sometimes the optimal values are not around well-identified values but are contained within larger domains with threshold values separating them. For instance, the difference between the LCD and the average pore size "delta\_pore" has a threshold value around \SI{0.3}{\angstrom} below which the contribution for the most selective structures (blue) is negative (see Figure~\ref{fgr:pdp_selection}); even though no clear correlations can be found, we can at least find a threshold value (about $0.23$) below which there is higher probability of having a high ambient-pressure selectivity. The same type of domain splits can be found for the average of krypton interaction energies distribution "mean\_grid\_krypton" (at around $15$), the Boltzmann weighted xenon interaction energies distribution "enthalpy\_std\_xenon" (at around $2.5$), the difference of exchange entropic term between the ambient temperature "delta\_TS0\_298\_900" (at around $3$) and high temperature and the effective number associated to the PSD "pore\_dist\_neff" (at around $2.3$). These domains separate structures that are selective at low pressure, which is key to telling apart the structures with a selectivity drop at ambient pressure from the ones without.

\begin{figure}[ht]
\centering
  \includegraphics[width=0.32\linewidth]{figures/4-ml/SDP/G_0.pdf}
  \includegraphics[width=0.32\linewidth]{figures/4-ml/SDP/G_900K.pdf}
  \includegraphics[width=0.32\linewidth]{figures/4-ml/SDP/D_i_vdw_uff298.pdf}
  \includegraphics[width=0.32\linewidth]{figures/4-ml/SDP/delta_G0_298_900.pdf}
  \includegraphics[width=0.32\linewidth]{figures/4-ml/SDP/delta_VF_18_20.pdf}
  \includegraphics[width=0.32\linewidth]{figures/4-ml/SDP/delta_pore.pdf}
\caption{Some SHAP dependence plots that are analyzed in the main article. The 18 top descriptors' SDPs can be found in the SI. }
  \label{fgr:pdp_selection}
\end{figure}

\subsection{Local interpretability}

%% local interpretation
To put into practice our previous analysis, let's look at some archetypal structures and how the model predicted the selectivity based on the descriptor values. We chose two MOF structures from the test set, their CSD code being respectively VIWMIZ and BIMDIL. Both structures are selective at low pressure but the first one decreases in selectivity while the other maintains it at ambient pressure. It will be interesting to see what the model does to tell apart these two completely different behaviors.

VIWMIZ is part of the highly selective structures that experience a selectivity drop at ambient pressure. If we convert back the free energy values to selectivity values, its selectivity is $62.8$ at infinite dilution and $14.5$ at ambient pressure. The ML model manages to give a close prediction of $12.0$ for the ambient-pressure selectivity based on the given values of the descriptors. If we only look at "G\_0", it has one of the most negative values, which explains the rather high negative contribution of $-1.81$. However, the $-0.57$ contribution of "G\_900K" is rather low compared to other materials (see Figure~\ref{fgr:pdp_selection}), since a value of $-4.05$ is not the most negative considering all structures. On the other hand, the remaining descriptors have values in the domain of positive contributions, which lead to the drop of the selectivity. For example, the difference of pore sizes "delta\_pore" has a value of \SI{1.38}{\angstrom} (above the threshold of \SI{0.23}{\angstrom}), which contributes $+0.25$ to the predicted selectivity and is consistent with the value ranges of the associated dependence plot. By reporting the values to the dependence plots, the same analyses can be made on the other positive contributions of the Figure~\ref{fgr:contribution}: "pore\_dist\_std" is above the threshold of $0.4$, "enthalpy\_std\_krypton" is above \SI{2.5}{\kilo\joule\per\mole}, "pore\_dist\_neff" is above $2.3$, "delta\_TS0\_298\_900" is below \SI{3}{\kilo\joule\per\mole} and "enthalpy\_modality" is around $0.75$ where positive contributions are more commonly observed. However, the "delta\_G0\_298\_900" value is a bit too close to its optimal value, which explains its negative contribution in this particular prediction. The rest of the features have almost negligible contributions and are detailed in the Figure~S11. By analyzing the contributions of each descriptor to the prediction given by our model, we can understand the underlying features of the VIWMIZ structure that explains the selectivity drop at higher pressure. The shape of the xenon and krypton energy distributions ("enthalpy\_std\_krypton" and "enthalpy\_modality") and of the PSD ("pore\_dist\_std" and "pore\_dist\_neff" ) as well as the void fraction difference "delta\_pore" are key descriptors at the origin of the lower selectivity at ambient pressure compared to the ideal infinite dilution case. Intuitively, one can easily understand that effective number of pores exceeding 2 can mean the presence of different pore sizes, which is consistent with the presence of pores that are less attractive to the xenon and leads necessarily to less selectivity. The previous statement is also very much consistent with a high standard deviation of the PSD or the Boltzmann weighted krypton interaction energy distribution. One can also conceive that a much larger difference between the average pore size and the LCD could mean a high disparity in pore sizes that leads to the presence of larger pores more and more loaded as the pressure rises.
The entropic term is however way more complex to interpret and opens unexplored ways of tackling the problem of selectivity drop at higher pressure unraveled by our previous study\autocite{Ren_2021}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/4-ml/main/VIWMIZ_clean.pdf}
      \caption{VIWMIZ: true $\Delta\e{exc} G_1=-6.63$}
    \end{subfigure}
         \hfill
    \begin{subfigure}[b]{0.47\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/4-ml/main/BIMDIL_clean.pdf}
      \caption{BIMDIL: true $\Delta\e{exc} G_1=-9.20$}
    \end{subfigure}
  \caption{Main contributions of the descriptors on the selectivity prediction of two archetypal examples. The descriptor labels used are detailed in the Table~S1 and S2 of the SI.}
  \label{fgr:contribution}
\end{figure}

The second structure BIMDIL is also among the most selective with a selectivity at low pressure of $41.0$, while maintaining it to $41.2$ at ambient pressure. The model manages to predict this stability of the selectivity by giving a value of $40.0$. Consequently, the first contribution of "G\_0" is among the most negative ones and set a baseline of $-2.4$ for the upcoming contributions. The contributions of "G\_900K" and "G\_900K" are not the highest possible but they continue to lower down the value of the predicted selectivity. It is the joint contributions of the other descriptors that will really discriminate between the two structures and decide why this one will keep its selectivity. Unlike the previously analyzed structure, this one has a "delta\_pore" value below \SI{0.3}{\angstrom}, which explains the negative Shapley value it has for our prediction. The contribution of "delta\_G0\_298\_900" that was only a little negative for the other one, is now playing a major role since it is right within the range of between $4$ and \SI{6}{\kilo\joule\per\mole} (see Figure~\ref{fgr:contribution}). We can also verify that "pore\_dist\_std" is now below the threshold instead of being above for the other structure. We can confirm that the other contributions are also following the rules implied by the SHAP dependence plots, no apparent anomalies are detected, and the joint efforts of all the descriptors tend to give a lower free energy value, which leads to the conservation of the selectivity value at higher pressure. The set of descriptor values is clearly very different from the previous structure, many values are in opposite contribution domains, which explains how the model manages to disentangle the highly selective structures to find out the ones that would keep their selectivity at higher pressure.

%% conclusions + sum-up of some qualitative rules
These two examples allow us to understand a bit more how the model tells apart the structures that will lose selectivity at higher pressure from the ones that will not. Most of the dependence plots can give very strong association between the descriptors and their effects; the outliers are rare enough that the inner logic of our model can be understood. As developed previously, the first three descriptors set a baseline on few information on the eventual drop of selectivity; then the other descriptors contribution is either positive, negligible or negative depending on the domain of values the descriptor is in. For instance, the average pore size and the largest cavity diameter need to be around very specific values to maximize the chance of keeping the selectivity at higher pressure, which was highlighted by previous works that emphasize on the importance of pore sizes close to the size of xenon for Xe/Kr separation.\autocite{Simon_2015, Ren_2021} The difference of entropy between the ambient temperature and \SI{900}{\kelvin} is surprising descriptor that separates selective structures depending on whether its value is within a given range. The difference of void fraction occupied by xenon and krypton is also very interesting since it affects the selectivity differently depending on whether it is highly selective or not, and the contribution is more or less proportional to its value. Different ways of measuring the disparity of the PSD and interaction energy distribution are key in sorting highly selective structures (in blue on the dependence plot Figure~\ref{fgr:pdp_selection}) between the ones maintaining their performance and the ones decreasing in selectivity. Among others, we can find the difference between the average pore size and the LCD, as well as the standard deviation of the PSD or of the Boltzmann weighted energy distribution that would behave very differently according to the domain in which the value lies. The SHAP dependence plots, partially plotted in the main text and entirely available in the SI, are very valuable reading grid to understand the mechanisms behind our ML model and more broadly to what it understood from the origins of Xe/Kr separation.

\subsection{Conclusions and perspectives}

In order to better understand separation processes inside nanoporous materials, we performed a machine learning prediction of Xe/Kr ambient-pressure selectivity that is faster than standard GCMC calculations. For MOF structures of the CoRE MOF 2019 database, a xenon/krypton selectivity evaluation would take less than a minute, while an equivalent GCMC calculation takes around \SI{40}{\minute}. Unlike most of the selectivity predictions of the literature, we chose to predict a selectivity in the logarithmic scale, because it focuses more on the order magnitude than the exact value of the selectivity of highly selective materials. Moreover, the conversion to an exchange Gibbs free energy allows a more thermodynamic approach based on enthalpy, entropy and free energy values. The challenge was then to predict a free energy equivalent of the ambient-pressure selectivity by using the low-pressure selectivity along with key energy, geometrical and chemical descriptors. The final, fully optimized ML model performs very well with an RMSE of \SI{0.36}{\kilo\joule\per\mole}, which corresponds to a $0.06$ RMSE on the base-10 log of the selectivity.

One of our more specific goals was to uncover underlying reasons of a selectivity drop at high pressure observed on some highly selective materials at low pressure. Previous studies found that a high diversity of pore sizes and channel sizes that favor adsorbate reorganizations could be at the origin of this phenomenon.\autocite{Ren_2021} By applying interpretability tools, we found quantitative factors that explain the conservation or the drop of the selectivity for highly selective materials. Depending on energy averaging at \SI{900}{\kelvin}, on statistical characterizations of the energy or pore size distributions, and on the difference of volumes occupiable we have a structure either with a selectivity similar to the low-pressure case or that is less selective at higher pressure. All the quantitative rules are contained in a complex ensemble of decision trees constructed by our XGBoost model, and they can be extracted to build rule of thumbs in order to back our intuition on the Xe/Kr selectivity in MOF structures.

%% Framework could be reproduced for other type of applications
The final ML model can be used in a well-designed workflow to find the best performing materials. For instance, we could filter out the structures with pores that cannot fit a xenon in, then we could use a first calculation of the low-pressure selectivity to filter out the selectivity below a given threshold. Finally, we can use the model to remove the structures that would experience a selectivity drop. We tested our methodology on the Xe/Kr separation as proof of concept since it is one of the simplest adsorption systems (monoatomic species with no electrostatic interactions). A similar approach can be generalized to other separation applications by calculating the infinite dilution energies with a more standard method (\emph{e.g.} Widom's insertion) and by adjusting the descriptor definitions to fit the adsorbates of interest.

%% Drawbacks + perspectives
This study ambitions to add new descriptor ideas to help the development of ever more efficient screening methodologies to find the best materials for target applications. However, like many other studies on the topic, this one also relies on a few strong assumptions --- the simulations are performed in rigid frameworks with non-polarized classical force fields. As suggested in the literature, the most selective materials ever synthesized for Xe/Kr separation are all based on the effect of open-metal sites that uses the difference of polarizability between the two molecules to efficiently separate them.\autocite{Li_2019, Pei_2022} Moreover, the structures can be made flexible using flexible force fields with adapted simulation methodologies\autocite{Bousquet2012} or by using multiple rigid simulations of snapshots from NPT simulations\autocite{Witman_2017}.
It would be possible to improve the simulations at the cost of CPU times, if we coupled it with a reduction of simulation time like the one presented in this article. The quest of ever-faster evaluation tools will allow us to investigate more complex properties and uncover structures with ever more interesting characteristics.

\textbf{Data Availability:} \url{}

\OnlyInSubfile{\printglobalbibliography}

\end{document}
